{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPceFAlrE1SjZtaUEy+sY13"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-5nRfBBIeMLl","executionInfo":{"status":"ok","timestamp":1717599602033,"user_tz":-180,"elapsed":2074,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"a4fb285f-bc4f-4208-e47b-50deecf37567"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import sys\n","import os\n","\n","repo_path = '/content/drive/MyDrive/Colab Notebooks/thesis_work/llm-for-rec'\n","sys.path.append(repo_path)"],"metadata":{"id":"M2MzWfKVsxh_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q -r '{repo_path}/requirements/requirements.txt'"],"metadata":{"id":"EmviU9ZtpxdX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd '{repo_path}'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MBbEjcz_xLUv","executionInfo":{"status":"ok","timestamp":1717599613655,"user_tz":-180,"elapsed":7,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"308be2c4-e294-4f36-ac7a-661835a36e1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/thesis_work/llm-for-rec\n"]}]},{"cell_type":"code","source":["from llm4rec.pipelines import RecBolePipelineRecommender\n","from llm4rec.dataset import RecboleSeqDataset\n","from llm4rec.evaluation.trainer import PipelineTrainer\n","from llm4rec.utils.dataset_utils import ml100k_preprocess\n","from recbole.data.utils import data_preparation\n","from recbole.config import Config\n","import os\n","import torch\n","\n","\n","model_cls = RecBolePipelineRecommender\n","dataset_name = 'ml-100k'\n","\n","config = Config(model=model_cls, dataset=dataset_name,\n","             config_file_list=['./llm4rec/configs/dataset_ml100k.yaml',\n","                               './llm4rec/configs/overall.yaml'])\n","\n","dataset = RecboleSeqDataset(config, preprocess_text_fn=ml100k_preprocess)\n","train_data, _, test_data = data_preparation(config, dataset)"],"metadata":{"id":"UX58lXXDZqnG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717599648760,"user_tz":-180,"elapsed":35110,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"549818d3-8c76-432b-e4af-a5e97875ca40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:command line args [-f /root/.local/share/jupyter/runtime/kernel-922764b6-1edf-4f44-8a97-543cdc29b02c.json] will not be used in RecBole\n"]}]},{"cell_type":"code","source":["from langchain_groq import ChatGroq\n","from dotenv import load_dotenv\n","from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n","from langchain import HuggingFaceHub\n","import torch\n","\n","embedding_size = 384\n","embedding_fn = HuggingFaceEmbeddings(\n","                model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"device\":\"cuda:0\" if torch.cuda.is_available() else \"cpu\"})\n","\n","path_to_env = os.path.join(repo_path, \"api_keys.env\")\n","load_dotenv(path_to_env)\n","\n","llm = ChatGroq(model_name=\"llama3-70b-8192\", temperature=0)\n","#llm = HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", model_kwargs={\"temperature\":0.1, \"max_new_tokens\":512,\n","#                                                                                  \"return_full_text\":False})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dpkvH-Oyu5th","executionInfo":{"status":"ok","timestamp":1717599656627,"user_tz":-180,"elapsed":7870,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"c4070df7-c7e4-4ed1-f0d8-1cb020d0b58f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm, trange\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["## Init memory"],"metadata":{"id":"wrXQFGnQ-JoM"}},{"cell_type":"code","source":["from llm4rec.memory import ItemMemory\n","from functools import partial\n","from langchain_community.document_loaders import WikipediaLoader\n","\n","wiki_loader = partial(WikipediaLoader, load_max_docs=1)\n","item_filepath = \"/content/drive/MyDrive/Colab Notebooks/thesis_work/llm-for-rec/examples/develop/item_memory_summ.json\"\n","\n","item_memory = ItemMemory(item_ids=dataset.item_id_token[1:],\n","                         title_col='movie_title',\n","                         dataset_info_map=dataset.item_token2attr,\n","                         load_filename=item_filepath,\n","                         summary_llm=llm,\n","                         augmentation_loader=wiki_loader)"],"metadata":{"id":"Sf3mL979pXaM","executionInfo":{"status":"ok","timestamp":1717599657695,"user_tz":-180,"elapsed":1072,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"76be4f15-1624-432e-93cb-4003896866a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1682/1682 [00:00<00:00, 592891.78it/s]\n"]}]},{"cell_type":"code","source":["from llm4rec.memory.user_long_term_memory import UserLongTermMemory\n","from llm4rec.memory.user_short_term_memory import UserShortTermMemory\n","from llm4rec.memory.base_memory import BaseMemory\n","from langchain_core.language_models.chat_models import BaseChatModel\n","from langchain_core.language_models.llms import BaseLLM\n","from langchain_core.embeddings import Embeddings\n","from recbole.data.dataset import Dataset\n","from tqdm import tqdm\n","\n","import numpy as np\n","import typing as tp\n","import os\n","\n","\n","class UserMemory:\n","    \"\"\"\n","    Stores and manipulates the historical data about user preferences and interactions.\n","    Can create user profile based on preferences.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        user_attributes: tp.Callable,\n","        short_term_limit: int,\n","        llm: tp.Union[BaseLLM, BaseChatModel],\n","        embeddings: Embeddings,\n","        emb_size: int,\n","        item_memory: BaseMemory,\n","        train_dataset: Dataset,\n","        min_rating: int = 1,\n","        max_rating: int = 5,\n","        num_to_retrieve: int = 3,\n","        update_long_term_every: int = None,\n","        load_filename: str = None,\n","    ):\n","        \"\"\"\n","        Initialized UserMemory\n","\n","        Args:\n","            user_attributes (Callable): A mapping fuction to get text user information from dataset.\n","            short_term_limit (int): Limit value for short-term memory.\n","            llm (BaseLLM, BaseChatModel): LLM model for reflecting on recent user preferences.\n","            embeddings (Embeddings): Embeddings model for retrieving of user long-term memory.\n","            emb_size (int): The dimension number of embedding vectors.\n","            item_memory (BaseMemory): Storage of item text information.\n","            train_dataset (Dataset): Dataset instance to initialize memory with dataset values.\n","            min_rating, max_rating: Values for scaling normalized rating values. Default range is 1-5\n","            num_to_retrieve (int): Number of retrieved relevant chunks from user long-term memory.\n","            update_long_term_every (int): How often to update long-term memory.\n","            load_filename (str): Complete file path to directory with saved memory values\n","        \"\"\"\n","        # global memory\n","        self.user_attributes = user_attributes\n","\n","        # personalized memory\n","        self.short_term_memory = UserShortTermMemory(\n","            llm=llm, item_memory=item_memory, memory_limit=short_term_limit\n","        )\n","        self.long_term_memory = UserLongTermMemory(\n","            embeddings=embeddings, emb_size=emb_size, k=num_to_retrieve\n","        )\n","        self.short_term_limit = short_term_limit\n","        self.update_long_term_every = (\n","            update_long_term_every if update_long_term_every else short_term_limit\n","        )\n","\n","        self.llm = llm\n","\n","        if load_filename is not None:\n","            self.load(load_filename)\n","        self._construct_memory(train_dataset, min_rating, max_rating)\n","\n","    def _construct_memory(\n","        self, train_dataset: Dataset, min_rating: int = 1, max_rating: int = 5\n","    ) -> None:\n","        \"\"\"\n","        Construct memory from values of train_dataset.\n","        \"\"\"\n","        history_item_matrix = train_dataset.history_item_matrix()\n","        inter_matrix = train_dataset.inter_matrix(\"csr\", value_field=\"rating\")\n","        user_id_mapping = lambda user_ids: train_dataset.id2token(\"user_id\", user_ids)\n","        item_id_mapping = lambda item_ids: train_dataset.id2token(\"item_id\", item_ids)\n","        history_matrix, _, history_lens = history_item_matrix\n","\n","        for user_id in tqdm(range(1, 350)): # len(history_matrix))):\n","            user_id_token = user_id_mapping(user_id)\n","\n","            if user_id_token not in self.short_term_memory.memory_store:\n","                user_history = history_matrix[user_id][: history_lens[user_id]].tolist()\n","                ratings = (\n","                    inter_matrix[user_id, :].toarray() * (max_rating - min_rating)\n","                    + min_rating\n","                )\n","                ratings = ratings.astype(\"int\")[0]\n","\n","                user_id_token = user_id_mapping(user_id)\n","                item_id_tokens = item_id_mapping(user_history)\n","\n","                for item, rating in zip(item_id_tokens, ratings):\n","                    self.update(\n","                        user_id_token, {\"rating\": int(rating), \"item_id\": str(item)}\n","                    )\n","\n","    def update(self, id: str, data: tp.Any) -> None:\n","        \"\"\"\n","        Update user memory.\n","        Long-term memory is updated every update_long_term_every times for user\n","        based on the number of performed updates.\n","        \"\"\"\n","        self.short_term_memory.update(id, data)\n","        update_counts = self.short_term_memory.get_update_counts(id)\n","\n","        if update_counts % self.update_long_term_every == 0:\n","            self.long_term_memory.update(id, self.short_term_memory.reflect(id))\n","\n","    def retrieve(self, id: str, query: str, memory_type: str = \"all\") -> tp.Any:\n","        \"\"\"\n","        Retrieve values from memory based on memory_type\n","        \"\"\"\n","        if memory_type == \"long\":\n","            return self.long_term_memory.retrieve(id, query)\n","        elif memory_type == \"short\":\n","            return self.short_term_memory.retrieve(id, query)\n","        elif memory_type == \"all\":\n","            return {\n","                \"long_term\": self.long_term_memory.retrieve(id, query),\n","                \"short_term\": self.short_term_memory.retrieve(id, query),\n","            }\n","\n","    def get_short_term_memory(self, id: str) -> tp.Any:\n","        return self.short_term_memory[id]\n","\n","    def get_long_term_memory(self, id: str) -> tp.Any:\n","        return self.long_term_memory[id]\n","\n","    def construct_user_profile(self, id: str, use_short_term: bool=False, use_long_term: bool=False) -> str:\n","        \"\"\"\n","        Create user profile information by concatenating available information\n","        about user from dataset, short-term memory reflection and long-term memory retrieved values.\n","        \"\"\"\n","        short_term_pref = self.short_term_memory.reflect(id)\n","        long_term_pref = self.retrieve(id, short_term_pref, memory_type=\"long\")\n","\n","        profile = f\"User {id}\"\n","        if self.user_attributes(id) != \"\":\n","            profile += f\" attributes: {self.user_attributes(id)}\\n\"\n","        else:\n","            profile += \"\\n\"\n","\n","        if use_short_term:\n","            update_counts = self.short_term_memory.get_update_counts(id)\n","            if (update_counts - 1) % self.update_long_term_every != 0:\n","                profile += f\"User recent preferences: {short_term_pref}\\n\"\n","        if use_long_term:\n","            profile += f\"User long-term preferences: {long_term_pref}.\"\n","\n","        #self.llm.invoke(\"Construct user profile using the following information: \")\n","        return profile\n","\n","    def save(self, folder_path: str) -> None:\n","        \"\"\"\n","        Save short-term memory and long-term memory to folder\n","        \"\"\"\n","        self.short_term_memory.save(folder_path + \"/short_term_mem.json\")\n","        self.long_term_memory.save(folder_path + \"/long_term_mem.json\")\n","\n","    def load(self, folder_path: str) -> None:\n","        \"\"\"\n","        Load short-term memory and long-term memory from folder\n","        \"\"\"\n","        assert os.path.exists(folder_path + \"/short_term_mem.json\")\n","        assert os.path.exists(folder_path + \"/long_term_mem.json\")\n","\n","        self.short_term_memory.load(folder_path + \"/short_term_mem.json\")\n","        self.long_term_memory.load(folder_path + \"/long_term_mem.json\")"],"metadata":{"id":"Jd5ifGdnzm5N","executionInfo":{"status":"ok","timestamp":1717607901284,"user_tz":-180,"elapsed":396,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["#from llm4rec.memory import UserMemory\n","\n","user_filepath = \"/content/drive/MyDrive/Colab Notebooks/thesis_work/llm-for-rec/examples/develop/user\"\n","\n","user_memory = UserMemory(train_dataset=train_data.dataset,\n","                         load_filename=user_filepath,\n","                         user_attributes=dataset.user_token2text,\n","                         short_term_limit=20, llm=llm,\n","                         embeddings=embedding_fn,\n","                         item_memory=item_memory,\n","                         emb_size=embedding_size)"],"metadata":{"id":"A0QWsoIPpOsL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f29b4083-f7c7-45c1-de4c-4c2fb9ec427c","executionInfo":{"status":"ok","timestamp":1717608670807,"user_tz":-180,"elapsed":763903,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":51,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:Max value of user's history interaction records has reached 43.612596553773024% of the total.\n","100%|██████████| 349/349 [12:43<00:00,  2.19s/it]\n"]}]},{"cell_type":"code","source":["user_memory.short_term_memory.memory_store"],"metadata":{"id":"SEyh_CI6shye","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["user_memory.long_term_memory.memory_store"],"metadata":{"id":"xFUtG-vl5mBW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["user_memory.save(user_filepath)"],"metadata":{"id":"FJRdTX-l4Zod","executionInfo":{"status":"ok","timestamp":1717608740699,"user_tz":-180,"elapsed":362,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":53,"outputs":[]}]}