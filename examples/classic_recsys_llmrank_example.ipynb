{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMZbZZdoJDQTlcdvKWVH5GH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Example of implementing LLMRank in our framework to run evaluation"],"metadata":{"id":"Wi6LsDQ47gSQ"}},{"cell_type":"markdown","source":["Run in google colaboratory"],"metadata":{"id":"q2SsUShgFUcf"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"PVsj9TqiahGl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717704508575,"user_tz":-180,"elapsed":1999,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"562595b8-31ae-4533-d731-f83b0ec4cc49"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import sys\n","import os\n","\n","repo_path = '/content/drive/MyDrive/Colab Notebooks/thesis_work/llm-for-rec'\n","sys.path.append(repo_path)"],"metadata":{"id":"VTFdx3QUbYpR","executionInfo":{"status":"ok","timestamp":1717704508575,"user_tz":-180,"elapsed":4,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Install requirements"],"metadata":{"id":"A6DQQIscFYT-"}},{"cell_type":"code","source":["!pip install -q -r '{repo_path}/requirements/requirements.txt'"],"metadata":{"id":"DpGmqMRNbbOB","executionInfo":{"status":"ok","timestamp":1717704530771,"user_tz":-180,"elapsed":22199,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Add config"],"metadata":{"id":"VzIZvvxzFctK"}},{"cell_type":"code","source":["import os\n","\n","config_dict = {\n","    \"csv_args\": {\"delimiter\": \"\\t\"},\n","    \"source_column\": \"item_id:token\",\n","    \"search_kwargs\": {\"k\": 20},\n","    \"data_path\": os.path.join(repo_path, \"datasets\"),\n","    \"load_col\": {\n","        \"inter\": [\"user_id\", \"item_id\", \"rating\", \"timestamp\"],\n","        \"item\": [\"item_id\", \"movie_title\"],\n","    },\n","    \"title_col\":\"movie_title\",\n","    \"text_col\": [\"movie_title\", \"release_year\", \"class\"],\n","    \"MAX_ITEM_LIST_LENGTH\": 10,\n","    \"eval_args\": {\"split\": {\"LS\": \"valid_and_test\"}, \"order\": \"TO\", \"mode\": \"full\"},\n","    \"repeatable\": True,\n","    \"loss_type\": \"CE\",\n","    \"train_batch_size\": 100,\n","    \"eval_batch_size\": 8,\n","    \"valid_metric\": \"NDCG@10\",\n","    \"metrics\": [\"Recall\", \"NDCG\"],\n","    \"topk\": [1, 5, 20],\n","    \"train_neg_sample_args\": None,\n","}"],"metadata":{"id":"VyhSfX0xbdCy","executionInfo":{"status":"ok","timestamp":1717704530772,"user_tz":-180,"elapsed":6,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["assert config_dict['search_kwargs']['k'] >= max(config_dict['topk'])"],"metadata":{"id":"IUi70Hftbe6_","executionInfo":{"status":"ok","timestamp":1717704530772,"user_tz":-180,"elapsed":5,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Get dataset and config"],"metadata":{"id":"eGJTExzKFibR"}},{"cell_type":"code","source":["# preprocessing for ml-100k\n","def ml100k_preprocess(text: str) -> str:\n","    if text.endswith(', The'):\n","        text = 'The ' + text[:-5]\n","    elif text.endswith(', A'):\n","        text = 'A ' + text[:-3]\n","    return text"],"metadata":{"id":"i8yDIgjmC6MZ","executionInfo":{"status":"ok","timestamp":1717704530772,"user_tz":-180,"elapsed":5,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from llm4rec.pipelines import RecBolePipelineRecommender\n","from llm4rec.dataset import RecboleSeqDataset\n","from llm4rec.evaluation.trainer import PipelineTrainer\n","from recbole.data.utils import data_preparation\n","from recbole.config import Config\n","import os\n","import torch\n","\n","model_cls = RecBolePipelineRecommender\n","dataset_name = 'ml-100k'\n","\n","config = Config(model=model_cls, dataset=dataset_name,\n","            config_dict=config_dict)\n","\n","dataset = RecboleSeqDataset(config, preprocess_text_fn=ml100k_preprocess)\n","train_data, _, eval_data = data_preparation(config, dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RtUGkHU3biap","executionInfo":{"status":"ok","timestamp":1717705292031,"user_tz":-180,"elapsed":1599,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"fee976da-d8f2-41d2-8fdd-be92c0211ce5"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:command line args [-f /root/.local/share/jupyter/runtime/kernel-33596485-d42f-4e11-845d-233831e8985e.json] will not be used in RecBole\n"]}]},{"cell_type":"markdown","source":["## Initialize models"],"metadata":{"id":"05LdWTAw7-uz"}},{"cell_type":"markdown","source":["Create instance of tradional sequential recsys model SASRec from RecBole implementation"],"metadata":{"id":"5-xo2c898L5h"}},{"cell_type":"code","source":["from recbole.model.sequential_recommender import SASRec\n","from llm4rec.tasks import SequentialRecBoleModelWrapper\n","\n","sas_model = SequentialRecBoleModelWrapper(SASRec, config.final_config_dict, train_data.dataset,\n","                                      n_layers=2, n_heads=2, embedding_size=64, hidden_size=64,\n","                                      inner_size=256, hidden_dropout_prob=0.5, attn_dropout_prob=0.5,\n","                                      hidden_act='gelu', layer_norm_eps=1e-12, initializer_range=0.02,\n","                                      loss_type='CE',\n","                                      pretrained_file=os.path.join(repo_path, 'examples', 'SASRec-ml-1m.pth'),\n","                                      top_k=20)"],"metadata":{"id":"cOb4hSXTafby","executionInfo":{"status":"ok","timestamp":1717704555173,"user_tz":-180,"elapsed":6,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Create instance of LLM and LLM Ranker model"],"metadata":{"id":"2EqTt7f_8ON7"}},{"cell_type":"code","source":["from langchain import HuggingFaceHub\n","from langchain_groq import ChatGroq\n","from dotenv import load_dotenv\n","from llm4rec.tasks import RankerRecommender\n","import os\n","\n","path_to_openai_env = os.path.join(repo_path, \"api_keys.env\")\n","load_dotenv(path_to_openai_env)\n","\n","llm = ChatGroq(model_name=\"llama3-70b-8192\", temperature=0)\n","#llm = HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", model_kwargs={\"temperature\":0.1, \"max_length\":512})\n","ranker = RankerRecommender(llm=llm, item2text=dataset.item_token2text)"],"metadata":{"id":"SFWPbvZecPnz","executionInfo":{"status":"ok","timestamp":1717704556548,"user_tz":-180,"elapsed":1380,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Combine steps in pipeline"],"metadata":{"id":"HQ6kXQjl8UZj"}},{"cell_type":"code","source":["from llm4rec.pipelines import RecBolePipelineRecommender\n","\n","model = RecBolePipelineRecommender(config=config,\n","                                    dataset=dataset,\n","                                    tasks=[sas_model, ranker],\n","                                    verbose=False)"],"metadata":{"id":"34wOBkwNfYzx","executionInfo":{"status":"ok","timestamp":1717705107747,"user_tz":-180,"elapsed":314,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["## Test on one user data"],"metadata":{"id":"1piF2Mmj7Yj9"}},{"cell_type":"markdown","source":["Enable debug mode for testing"],"metadata":{"id":"IKm3kQaN-Br8"}},{"cell_type":"code","source":["from langchain.globals import set_debug\n","\n","set_debug(True)\n","model.verbose=True"],"metadata":{"id":"WUkL3NMd98Mi","executionInfo":{"status":"ok","timestamp":1717704556548,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["for batched_data in eval_data:\n","    interaction, history_index, positive_u, positive_i = batched_data\n","    batch_size = len(interaction[\"user_id\"])\n","\n","    for inter_idx in range(batch_size):\n","        user_id = interaction[inter_idx][\"user_id\"]\n","        history_ids = interaction[inter_idx][\"item_id_list\"]\n","        history_length = min(\n","            config[\"MAX_ITEM_LIST_LENGTH\"],\n","            interaction[inter_idx][\"item_length\"],\n","        )\n","        history_names = [eval_data.dataset.item_id2text(hist_id) for hist_id in history_ids[:history_length].tolist()]\n","        history_item_ids = eval_data.dataset.id2token(\"item_id\", history_ids[:history_length])\n","        user_token_id  = eval_data.dataset.id2token('user_id', user_id)\n","\n","        reco = model.recommend(user_token=user_token_id, prev_interactions=history_item_ids)\n","        print(reco)\n","        break\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VdmM0OolcuYz","executionInfo":{"status":"ok","timestamp":1717705084536,"user_tz":-180,"elapsed":3725,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"d92f7fec-4051-4288-a6bb-ad5897de131d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Task 1 outputs:  ['39', '1613', '290', '20', '668', '452', '1094', '1682', '603', '1487', '1271', '740', '164', '57', '1376', '982', '281', '225', '1057', '1100']\n","\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatGroq] Entering LLM run with input:\n","\u001b[0m{\n","  \"prompts\": [\n","    \"Human: I've been interested in the following items in the past in order:\\nmovie_title:The Empire Strikes Back; release_year:1980; class:Action Adventure Drama Romance Sci-Fi War,\\nmovie_title:Beautiful Girls; release_year:1996; class:Drama,\\nmovie_title:Mars Attacks!; release_year:1996; class:Action Comedy Sci-Fi War,\\nmovie_title:Broken Arrow; release_year:1996; class:Action Thriller,\\nmovie_title:Amistad; release_year:1997; class:Drama,\\nmovie_title:The Long Kiss Goodnight; release_year:1996; class:Action Thriller,\\nmovie_title:French Kiss; release_year:1995; class:Comedy Romance,\\nmovie_title:The Maltese Falcon; release_year:1941; class:Film-Noir Mystery,\\nmovie_title:Dazed and Confused; release_year:1993; class:Comedy,\\nmovie_title:Strange Days; release_year:1995; class:Action Crime Sci-Fi.\\n\\nNow there are 20 candidate items that I may be interested in:\\nmovie_title:Strange Days; release_year:1995; class:Action Crime Sci-Fi,\\nmovie_title:Tokyo Fist; release_year:1995; class:Action Drama,\\nmovie_title:Fierce Creatures; release_year:1997; class:Comedy,\\nmovie_title:Angels and Insects; release_year:1995; class:Drama Romance,\\nmovie_title:Blood Beach; release_year:1981; class:Action Horror,\\nmovie_title:Jaws 2; release_year:1978; class:Action Horror,\\nmovie_title:A Thin Line Between Love and Hate; release_year:1996; class:Comedy,\\nmovie_title:Scream of Stone (Schrei aus Stein); release_year:1991; class:Drama,\\nmovie_title:Rear Window; release_year:1954; class:Mystery Thriller,\\nmovie_title:Even Cowgirls Get the Blues; release_year:1993; class:Comedy Romance,\\nmovie_title:North; release_year:1994; class:Comedy,\\nmovie_title:Jane Eyre; release_year:1996; class:Drama Romance,\\nmovie_title:The Abyss; release_year:1989; class:Action Adventure Sci-Fi Thriller,\\nmovie_title:Priest; release_year:1994; class:Drama,\\nmovie_title:Meet Wally Sparks; release_year:1997; class:Comedy,\\nmovie_title:Maximum Risk; release_year:1996; class:Action Adventure Thriller,\\nmovie_title:The River Wild; release_year:1994; class:Action Thriller,\\nmovie_title:101 Dalmatians; release_year:1996; class:Children's Comedy,\\nmovie_title:The Pallbearer; release_year:1996; class:Comedy,\\nmovie_title:What Happened Was...; release_year:1994; class:Comedy Drama Romance.\\nPlease rank these 20 items by measuring the possibilities that I would like to interact with next most, according to my interest history. Please think step by step.\\nPlease show me your ranking results with item titles and order numbers. Split your output with line break. You MUST rank the given candidate items. You can not generate items that are not in the given candidate list. You MUST NOT include items the user has already interacted with.\"\n","  ]\n","}\n","\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatGroq] [3.14s] Exiting LLM run with output:\n","\u001b[0m{\n","  \"generations\": [\n","    [\n","      {\n","        \"text\": \"Based on your interest history, I will rank the 20 candidate items by measuring the possibilities that you would like to interact with next most. Here are the results:\\n\\n1. **The Abyss** (1989) - Action Adventure Sci-Fi Thriller\\n2. **Rear Window** (1954) - Mystery Thriller\\n3. **The River Wild** (1994) - Action Thriller\\n4. **Maximum Risk** (1996) - Action Adventure Thriller\\n5. **Tokyo Fist** (1995) - Action Drama\\n6. **Angels and Insects** (1995) - Drama Romance\\n7. **Jane Eyre** (1996) - Drama Romance\\n8. **Priest** (1994) - Drama\\n9. **What Happened Was...** (1994) - Comedy Drama Romance\\n10. **Even Cowgirls Get the Blues** (1993) - Comedy Romance\\n11. **North** (1994) - Comedy\\n12. **Fierce Creatures** (1997) - Comedy\\n13. **Meet Wally Sparks** (1997) - Comedy\\n14. **The Pallbearer** (1996) - Comedy\\n15. **A Thin Line Between Love and Hate** (1996) - Comedy\\n16. **Scream of Stone (Schrei aus Stein)** (1991) - Drama\\n17. **Blood Beach** (1981) - Action Horror\\n18. **Jaws 2** (1978) - Action Horror\\n19. **101 Dalmatians** (1996) - Children's Comedy\\n20. **Scream of Stone (Schrei aus Stein)** (1991) - Drama\\n\\nNote that the ranking is based on the similarity of genres and release years with your interest history. The top-ranked items are those that have a higher likelihood of matching your interests.\",\n","        \"generation_info\": {\n","          \"finish_reason\": \"stop\",\n","          \"logprobs\": null\n","        },\n","        \"type\": \"ChatGeneration\",\n","        \"message\": {\n","          \"lc\": 1,\n","          \"type\": \"constructor\",\n","          \"id\": [\n","            \"langchain\",\n","            \"schema\",\n","            \"messages\",\n","            \"AIMessage\"\n","          ],\n","          \"kwargs\": {\n","            \"content\": \"Based on your interest history, I will rank the 20 candidate items by measuring the possibilities that you would like to interact with next most. Here are the results:\\n\\n1. **The Abyss** (1989) - Action Adventure Sci-Fi Thriller\\n2. **Rear Window** (1954) - Mystery Thriller\\n3. **The River Wild** (1994) - Action Thriller\\n4. **Maximum Risk** (1996) - Action Adventure Thriller\\n5. **Tokyo Fist** (1995) - Action Drama\\n6. **Angels and Insects** (1995) - Drama Romance\\n7. **Jane Eyre** (1996) - Drama Romance\\n8. **Priest** (1994) - Drama\\n9. **What Happened Was...** (1994) - Comedy Drama Romance\\n10. **Even Cowgirls Get the Blues** (1993) - Comedy Romance\\n11. **North** (1994) - Comedy\\n12. **Fierce Creatures** (1997) - Comedy\\n13. **Meet Wally Sparks** (1997) - Comedy\\n14. **The Pallbearer** (1996) - Comedy\\n15. **A Thin Line Between Love and Hate** (1996) - Comedy\\n16. **Scream of Stone (Schrei aus Stein)** (1991) - Drama\\n17. **Blood Beach** (1981) - Action Horror\\n18. **Jaws 2** (1978) - Action Horror\\n19. **101 Dalmatians** (1996) - Children's Comedy\\n20. **Scream of Stone (Schrei aus Stein)** (1991) - Drama\\n\\nNote that the ranking is based on the similarity of genres and release years with your interest history. The top-ranked items are those that have a higher likelihood of matching your interests.\",\n","            \"response_metadata\": {\n","              \"token_usage\": {\n","                \"completion_tokens\": 391,\n","                \"prompt_tokens\": 697,\n","                \"total_tokens\": 1088,\n","                \"completion_time\": 1.152451763,\n","                \"prompt_time\": 0.318329879,\n","                \"queue_time\": null,\n","                \"total_time\": 1.470781642\n","              },\n","              \"model_name\": \"llama3-70b-8192\",\n","              \"system_fingerprint\": \"fp_2f30b0b571\",\n","              \"finish_reason\": \"stop\",\n","              \"logprobs\": null\n","            },\n","            \"type\": \"ai\",\n","            \"id\": \"run-cb42e63c-2873-4e45-8f4f-69bddb89c458-0\",\n","            \"tool_calls\": [],\n","            \"invalid_tool_calls\": []\n","          }\n","        }\n","      }\n","    ]\n","  ],\n","  \"llm_output\": {\n","    \"token_usage\": {\n","      \"completion_tokens\": 391,\n","      \"prompt_tokens\": 697,\n","      \"total_tokens\": 1088,\n","      \"completion_time\": 1.152451763,\n","      \"prompt_time\": 0.318329879,\n","      \"queue_time\": null,\n","      \"total_time\": 1.470781642\n","    },\n","    \"model_name\": \"llama3-70b-8192\",\n","    \"system_fingerprint\": \"fp_2f30b0b571\"\n","  },\n","  \"run\": null\n","}\n","Task 2 outputs:  ['164', '603', '281', '982', '1613', '20', '740', '57', '1100', '1487', '1271', '290', '1376', '1057', '1094', '668', '452', '225', '39', '1682']\n","['164', '603', '281', '982', '1613', '20', '740', '57', '1100', '1487', '1271', '290', '1376', '1057', '1094', '668', '452', '225', '39', '1682']\n"]}]},{"cell_type":"code","source":["set_debug(False)\n","model.verbose = False"],"metadata":{"id":"BOb66vVbAVna"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate pipeline"],"metadata":{"id":"5Bnv1prT7T_H"}},{"cell_type":"code","source":["trainer = PipelineTrainer(config, model)\n","test_result = trainer.evaluate(eval_data, show_progress=config['show_progress'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"_1iwXyVggL5s","executionInfo":{"status":"error","timestamp":1716818186324,"user_tz":-180,"elapsed":1638792,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"d8368955-7768-4587-bbd6-9cc3aa718b7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[1;35mEvaluate   \u001b[0m:   7%|███▏                                           | 8/118 [27:18<6:15:27, 204.80s/it]\u001b[0m\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# thrown on 4xx and 5xx status code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-35a29392356c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'show_progress'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-f728651e3f67>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_data, show_progress)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;31m# model part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 candidates = self.model.run(\n\u001b[0m\u001b[1;32m     68\u001b[0m                     \u001b[0muser_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_token_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mprev_interactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprev_interactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/thesis_work/llm-for-rec/llm4rec/pipelines/pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **inputs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtask_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_names\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtask_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/thesis_work/llm-for-rec/llm4rec/tasks/ranker/general_ranker.py\u001b[0m in \u001b[0;36mrecommend\u001b[0;34m(self, prev_interactions, candidates, user_profile)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"My profile: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0muser_profile\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'.\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mranked_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_items_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         return cast(\n\u001b[1;32m    169\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    598\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     async def agenerate_prompt(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         flattened_outputs = [\n\u001b[1;32m    458\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 results.append(\n\u001b[0;32m--> 446\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    447\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    672\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         }\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    281\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \"\"\"\n\u001b[0;32m--> 283\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         )\n\u001b[0;32m-> 1225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 920\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    921\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1004\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0;31m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;31m# different thread if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m         return self._request(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["test_result"],"metadata":{"id":"b8Dd4pgpjvnd"},"execution_count":null,"outputs":[]}]}