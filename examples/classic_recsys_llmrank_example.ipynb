{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMDq8VWD/cQ6bfjQG2CbNpZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Example of implementing LLMRank in our framework to run evaluation"],"metadata":{"id":"Wi6LsDQ47gSQ"}},{"cell_type":"markdown","source":["Run in google colaboratory"],"metadata":{"id":"q2SsUShgFUcf"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"PVsj9TqiahGl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716813155091,"user_tz":-180,"elapsed":2480,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"e1df8de9-17f3-49fd-f68c-4c020288099c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import sys\n","import os\n","\n","repo_path = '/content/drive/MyDrive/Colab Notebooks/thesis_work/llm-for-rec'\n","sys.path.append(repo_path)"],"metadata":{"id":"VTFdx3QUbYpR","executionInfo":{"status":"ok","timestamp":1716813155091,"user_tz":-180,"elapsed":3,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Install requirements"],"metadata":{"id":"A6DQQIscFYT-"}},{"cell_type":"code","source":["!pip install -q -r '{repo_path}/requirements/requirements.txt'"],"metadata":{"id":"DpGmqMRNbbOB","executionInfo":{"status":"ok","timestamp":1716813179411,"user_tz":-180,"elapsed":24322,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Add config"],"metadata":{"id":"VzIZvvxzFctK"}},{"cell_type":"code","source":["import os\n","\n","config_dict = {\n","    \"csv_args\": {\"delimiter\": \"\\t\"},\n","    \"source_column\": \"item_id:token\",\n","    \"search_kwargs\": {\"k\": 20},\n","    \"data_path\": os.path.join(repo_path, \"datasets\"),\n","    \"load_col\": {\n","        \"inter\": [\"user_id\", \"item_id\", \"rating\", \"timestamp\"],\n","        \"item\": [\"item_id\", \"movie_title\"],\n","    },\n","    \"title_col\":\"movie_title\",\n","    \"text_col\": [\"movie_title\", \"release_year\", \"class\"],\n","    \"MAX_ITEM_LIST_LENGTH\": 10,\n","    \"eval_args\": {\"split\": {\"LS\": \"valid_and_test\"}, \"order\": \"TO\", \"mode\": \"full\"},\n","    \"repeatable\": True,\n","    \"loss_type\": \"CE\",\n","    \"train_batch_size\": 100,\n","    \"eval_batch_size\": 8,\n","    \"valid_metric\": \"NDCG@10\",\n","    \"metrics\": [\"Recall\", \"NDCG\"],\n","    \"topk\": [1, 5, 20],\n","    \"train_neg_sample_args\": None,\n","}"],"metadata":{"id":"VyhSfX0xbdCy","executionInfo":{"status":"ok","timestamp":1716813671743,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["assert config_dict['search_kwargs']['k'] >= max(config_dict['topk'])"],"metadata":{"id":"IUi70Hftbe6_","executionInfo":{"status":"ok","timestamp":1716813672706,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["## Get dataset and config"],"metadata":{"id":"eGJTExzKFibR"}},{"cell_type":"code","source":["# preprocessing for ml-100k\n","def ml100k_preprocess(text: str) -> str:\n","    if text.endswith(', The'):\n","        text = 'The ' + text[:-5]\n","    elif text.endswith(', A'):\n","        text = 'A ' + text[:-3]\n","    return text"],"metadata":{"id":"i8yDIgjmC6MZ","executionInfo":{"status":"ok","timestamp":1716813672706,"user_tz":-180,"elapsed":1,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["from llm4rec.pipelines import RecBolePipelineRecommender\n","from llm4rec.dataset import RecboleSeqDataset\n","from llm4rec.trainer import PipelineTrainer\n","from recbole.data.utils import data_preparation\n","from recbole.config import Config\n","import os\n","import torch\n","\n","model_cls = RecBolePipelineRecommender\n","dataset_name = 'ml-100k'\n","\n","config = Config(model=model_cls, dataset=dataset_name,\n","            config_dict=config_dict)\n","\n","dataset = RecboleSeqDataset(config, preprocess_text_fn=ml100k_preprocess)\n","train_data, _, eval_data = data_preparation(config, dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RtUGkHU3biap","executionInfo":{"status":"ok","timestamp":1716813681815,"user_tz":-180,"elapsed":8063,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"80217842-10b2-4657-e058-44cc0550e7b0"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:command line args [-f /root/.local/share/jupyter/runtime/kernel-95829a15-7792-4f46-be3e-16be934c468e.json] will not be used in RecBole\n"]}]},{"cell_type":"markdown","source":["## Initialize models"],"metadata":{"id":"05LdWTAw7-uz"}},{"cell_type":"markdown","source":["Create instance of tradional sequential recsys model SASRec from RecBole implementation"],"metadata":{"id":"5-xo2c898L5h"}},{"cell_type":"code","source":["from recbole.model.sequential_recommender import SASRec\n","from llm4rec.tasks import SequentialRecBoleModelWrapper\n","\n","sas_model = SequentialRecBoleModelWrapper(SASRec, config.final_config_dict, train_data.dataset,\n","                                      n_layers=2, n_heads=2, embedding_size=64, hidden_size=64,\n","                                      inner_size=256, hidden_dropout_prob=0.5, attn_dropout_prob=0.5,\n","                                      hidden_act='gelu', layer_norm_eps=1e-12, initializer_range=0.02,\n","                                      loss_type='CE',\n","                                      pretrained_file=os.path.join(repo_path, 'examples', 'SASRec-ml-1m.pth'),\n","                                      top_k=20)"],"metadata":{"id":"cOb4hSXTafby","executionInfo":{"status":"ok","timestamp":1716813325372,"user_tz":-180,"elapsed":3,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["Create instance of LLM and LLM Ranker model"],"metadata":{"id":"2EqTt7f_8ON7"}},{"cell_type":"code","source":["from langchain import HuggingFaceHub\n","from langchain_groq import ChatGroq\n","from dotenv import load_dotenv\n","from llm4rec.tasks import RankerRecommender\n","import os\n","\n","path_to_openai_env = os.path.join(repo_path, \"api_keys.env\")\n","load_dotenv(path_to_openai_env)\n","\n","llm = ChatGroq(model_name=\"llama3-70b-8192\", temperature=0)\n","#llm = HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", model_kwargs={\"temperature\":0.1, \"max_length\":512})\n","ranker = RankerRecommender(llm=llm, item2text=dataset.item_token2text)"],"metadata":{"id":"SFWPbvZecPnz","executionInfo":{"status":"ok","timestamp":1716813689294,"user_tz":-180,"elapsed":1103,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["Combine steps in pipeline"],"metadata":{"id":"HQ6kXQjl8UZj"}},{"cell_type":"code","source":["from llm4rec.pipelines import RecBolePipelineRecommender\n","\n","model = RecBolePipelineRecommender(config=config,\n","                                    dataset=dataset,\n","                                    tasks=[sas_model, ranker],\n","                                    verbose=False)"],"metadata":{"id":"34wOBkwNfYzx","executionInfo":{"status":"ok","timestamp":1716813689295,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["## Test on one user data"],"metadata":{"id":"1piF2Mmj7Yj9"}},{"cell_type":"markdown","source":["Enable debug mode for testing"],"metadata":{"id":"IKm3kQaN-Br8"}},{"cell_type":"code","source":["from langchain.globals import set_debug\n","\n","set_debug(True)\n","model.verbose=True"],"metadata":{"id":"WUkL3NMd98Mi","executionInfo":{"status":"ok","timestamp":1716813331076,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["for batched_data in eval_data:\n","    interaction, history_index, positive_u, positive_i = batched_data\n","    batch_size = len(interaction[\"user_id\"])\n","\n","    for inter_idx in range(batch_size):\n","        user_id = interaction[inter_idx][\"user_id\"]\n","        history_ids = interaction[inter_idx][\"item_id_list\"]\n","        history_length = min(\n","            config[\"MAX_ITEM_LIST_LENGTH\"],\n","            interaction[inter_idx][\"item_length\"],\n","        )\n","        history_names = [eval_data.dataset.item_id2text(hist_id) for hist_id in history_ids[:history_length].tolist()]\n","        history_item_ids = eval_data.dataset.id2token(\"item_id\", history_ids[:history_length])\n","        user_token_id  = eval_data.dataset.id2token('user_id', user_id)\n","\n","        reco = model.run(user_token=user_token_id, prev_interactions=history_item_ids)\n","        print(reco)\n","        break\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VdmM0OolcuYz","executionInfo":{"status":"ok","timestamp":1716813333888,"user_tz":-180,"elapsed":1707,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"8c89af88-4ea9-4d8c-8b05-f69eca3d1244"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Task 1 outputs:  ['39' '859' '1367' '219' '1342' '512' '1677' '1368' '789' '961' '589'\n"," '133' '30' '1498' '1055' '1065' '1307' '1459' '1185' '314']\n","\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatGroq] Entering LLM run with input:\n","\u001b[0m{\n","  \"prompts\": [\n","    \"Human: I've been interested in the following items in the past in order:\\nmovie_title:Empire Strikes Back, The; release_year:1980; class:Action Adventure Drama Romance Sci-Fi War,\\nmovie_title:Beautiful Girls; release_year:1996; class:Drama,\\nmovie_title:Mars Attacks!; release_year:1996; class:Action Comedy Sci-Fi War,\\nmovie_title:Broken Arrow; release_year:1996; class:Action Thriller,\\nmovie_title:Amistad; release_year:1997; class:Drama,\\nmovie_title:Long Kiss Goodnight, The; release_year:1996; class:Action Thriller,\\nmovie_title:French Kiss; release_year:1995; class:Comedy Romance,\\nmovie_title:Maltese Falcon, The; release_year:1941; class:Film-Noir Mystery,\\nmovie_title:Dazed and Confused; release_year:1993; class:Comedy,\\nmovie_title:Strange Days; release_year:1995; class:Action Crime Sci-Fi.\\n\\nNow there are 20 candidate items that I may be interested in:\\nmovie_title:Strange Days; release_year:1995; class:Action Crime Sci-Fi,\\nmovie_title:April Fool's Day; release_year:1986; class:Comedy Horror,\\nmovie_title:Faust; release_year:1994; class:Animation,\\nmovie_title:Nightmare on Elm Street, A; release_year:1984; class:Horror,\\nmovie_title:Convent, The (Convento, O); release_year:1995; class:Drama,\\nmovie_title:Wings of Desire; release_year:1987; class:Comedy Drama Romance,\\nmovie_title:Sweet Nothing; release_year:1995; class:Drama,\\nmovie_title:Mina Tannenbaum; release_year:1994; class:Drama,\\nmovie_title:Swimming with Sharks; release_year:1995; class:Comedy Drama,\\nmovie_title:Orlando; release_year:1993; class:Drama,\\nmovie_title:Wild Bunch, The; release_year:1969; class:Western,\\nmovie_title:Gone with the Wind; release_year:1939; class:Drama Romance War,\\nmovie_title:Belle de jour; release_year:1967; class:Drama,\\nmovie_title:Farmer & Chase; release_year:1995; class:Comedy,\\nmovie_title:Simple Twist of Fate, A; release_year:1994; class:Drama,\\nmovie_title:Koyaanisqatsi; release_year:1983; class:Documentary War,\\nmovie_title:Carmen Miranda: Bananas Is My Business; release_year:1994; class:Documentary,\\nmovie_title:Madame Butterfly; release_year:1995; class:Musical,\\nmovie_title:In the Army Now; release_year:1994; class:Comedy War,\\nmovie_title:3 Ninjas: High Noon At Mega Mountain; release_year:1998; class:Action Children's.\\nPlease rank these 20 items by measuring the possibilities that I would like to interact with next most, according to my interest history. Please think step by step.\\nPlease show me your ranking results with item titles and order numbers. Split your output with line break. You MUST rank the given candidate items. You can not generate items that are not in the given candidate list. You MUST NOT include items the user has already interacted with.\"\n","  ]\n","}\n","\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatGroq] [1.55s] Exiting LLM run with output:\n","\u001b[0m{\n","  \"generations\": [\n","    [\n","      {\n","        \"text\": \"I'll rank the candidate items based on the similarity of their genres and release years to the items you've shown interest in. Here's the ranking:\\n\\n1. Strange Days (1995) - Action Crime Sci-Fi\\n2. Swimming with Sharks (1995) - Comedy Drama\\n3. Sweet Nothing (1995) - Drama\\n4. Farmer & Chase (1995) - Comedy\\n5. Convent, The (Convento, O) (1995) - Drama\\n6. Mina Tannenbaum (1994) - Drama\\n7. Simple Twist of Fate, A (1994) - Drama\\n8. Faust (1994) - Animation\\n9. Orlando (1993) - Drama\\n10. Wings of Desire (1987) - Comedy Drama Romance\\n11. April Fool's Day (1986) - Comedy Horror\\n12. Nightmare on Elm Street, A (1984) - Horror\\n13. Koyaanisqatsi (1983) - Documentary War\\n14. Belle de jour (1967) - Drama\\n15. Wild Bunch, The (1969) - Western\\n16. Gone with the Wind (1939) - Drama Romance War\\n17. In the Army Now (1994) - Comedy War\\n18. Carmen Miranda: Bananas Is My Business (1994) - Documentary\\n19. Madame Butterfly (1995) - Musical\\n20. 3 Ninjas: High Noon At Mega Mountain (1998) - Action Children's\\n\\nNote that the top-ranked items are from the 1990s, which is consistent with your interest history. The ranking also favors items with genres that overlap with your previous interests, such as Action, Comedy, Drama, and Sci-Fi.\",\n","        \"generation_info\": {\n","          \"finish_reason\": \"stop\",\n","          \"logprobs\": null\n","        },\n","        \"type\": \"ChatGeneration\",\n","        \"message\": {\n","          \"lc\": 1,\n","          \"type\": \"constructor\",\n","          \"id\": [\n","            \"langchain\",\n","            \"schema\",\n","            \"messages\",\n","            \"AIMessage\"\n","          ],\n","          \"kwargs\": {\n","            \"content\": \"I'll rank the candidate items based on the similarity of their genres and release years to the items you've shown interest in. Here's the ranking:\\n\\n1. Strange Days (1995) - Action Crime Sci-Fi\\n2. Swimming with Sharks (1995) - Comedy Drama\\n3. Sweet Nothing (1995) - Drama\\n4. Farmer & Chase (1995) - Comedy\\n5. Convent, The (Convento, O) (1995) - Drama\\n6. Mina Tannenbaum (1994) - Drama\\n7. Simple Twist of Fate, A (1994) - Drama\\n8. Faust (1994) - Animation\\n9. Orlando (1993) - Drama\\n10. Wings of Desire (1987) - Comedy Drama Romance\\n11. April Fool's Day (1986) - Comedy Horror\\n12. Nightmare on Elm Street, A (1984) - Horror\\n13. Koyaanisqatsi (1983) - Documentary War\\n14. Belle de jour (1967) - Drama\\n15. Wild Bunch, The (1969) - Western\\n16. Gone with the Wind (1939) - Drama Romance War\\n17. In the Army Now (1994) - Comedy War\\n18. Carmen Miranda: Bananas Is My Business (1994) - Documentary\\n19. Madame Butterfly (1995) - Musical\\n20. 3 Ninjas: High Noon At Mega Mountain (1998) - Action Children's\\n\\nNote that the top-ranked items are from the 1990s, which is consistent with your interest history. The ranking also favors items with genres that overlap with your previous interests, such as Action, Comedy, Drama, and Sci-Fi.\",\n","            \"response_metadata\": {\n","              \"token_usage\": {\n","                \"completion_tokens\": 363,\n","                \"prompt_tokens\": 723,\n","                \"total_tokens\": 1086,\n","                \"completion_time\": 1.065770214,\n","                \"prompt_time\": 0.147089176,\n","                \"queue_time\": null,\n","                \"total_time\": 1.21285939\n","              },\n","              \"model_name\": \"llama3-70b-8192\",\n","              \"system_fingerprint\": \"fp_753a4aecf6\",\n","              \"finish_reason\": \"stop\",\n","              \"logprobs\": null\n","            },\n","            \"type\": \"ai\",\n","            \"id\": \"run-981d6142-46a0-4eaa-8151-e4a321e597fa-0\",\n","            \"tool_calls\": [],\n","            \"invalid_tool_calls\": []\n","          }\n","        }\n","      }\n","    ]\n","  ],\n","  \"llm_output\": {\n","    \"token_usage\": {\n","      \"completion_tokens\": 363,\n","      \"prompt_tokens\": 723,\n","      \"total_tokens\": 1086,\n","      \"completion_time\": 1.065770214,\n","      \"prompt_time\": 0.147089176,\n","      \"queue_time\": null,\n","      \"total_time\": 1.21285939\n","    },\n","    \"model_name\": \"llama3-70b-8192\",\n","    \"system_fingerprint\": \"fp_753a4aecf6\"\n","  },\n","  \"run\": null\n","}\n","Task 2 outputs:  ['39', '789', '1677', '1498', '1368', '1055', '1367', '961', '512', '859', '219', '1065', '30', '589', '133', '1185', '1307', '1459', '314', '1342']\n","['39', '789', '1677', '1498', '1368', '1055', '1367', '961', '512', '859', '219', '1065', '30', '589', '133', '1185', '1307', '1459', '314', '1342']\n"]}]},{"cell_type":"code","source":["set_debug(False)\n","model.verbose = False"],"metadata":{"id":"BOb66vVbAVna","executionInfo":{"status":"ok","timestamp":1716813707337,"user_tz":-180,"elapsed":3,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate pipeline"],"metadata":{"id":"5Bnv1prT7T_H"}},{"cell_type":"code","source":["trainer = PipelineTrainer(config, model)\n","test_result = trainer.evaluate(eval_data, show_progress=config['show_progress'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"_1iwXyVggL5s","executionInfo":{"status":"error","timestamp":1716818186324,"user_tz":-180,"elapsed":1638792,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"d8368955-7768-4587-bbd6-9cc3aa718b7d"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[1;35mEvaluate   \u001b[0m:   7%|███▏                                           | 8/118 [27:18<6:15:27, 204.80s/it]\u001b[0m\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# thrown on 4xx and 5xx status code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-35a29392356c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'show_progress'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-f728651e3f67>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_data, show_progress)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;31m# model part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 candidates = self.model.run(\n\u001b[0m\u001b[1;32m     68\u001b[0m                     \u001b[0muser_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_token_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mprev_interactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprev_interactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/thesis_work/llm-for-rec/llm4rec/pipelines/pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **inputs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtask_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_names\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtask_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/thesis_work/llm-for-rec/llm4rec/tasks/ranker/general_ranker.py\u001b[0m in \u001b[0;36mrecommend\u001b[0;34m(self, prev_interactions, candidates, user_profile)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"My profile: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0muser_profile\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'.\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mranked_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_items_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         return cast(\n\u001b[1;32m    169\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    598\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     async def agenerate_prompt(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         flattened_outputs = [\n\u001b[1;32m    458\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 results.append(\n\u001b[0;32m--> 446\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    447\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    672\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         }\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    281\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \"\"\"\n\u001b[0;32m--> 283\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         )\n\u001b[0;32m-> 1225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 920\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    921\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1004\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0;31m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;31m# different thread if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m         return self._request(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["test_result"],"metadata":{"id":"b8Dd4pgpjvnd","executionInfo":{"status":"aborted","timestamp":1716813662997,"user_tz":-180,"elapsed":4,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":null,"outputs":[]}]}